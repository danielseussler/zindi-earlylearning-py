{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Strategy: Gradient Boosted Trees, specifically 'LightGBM'.\n",
    "\n",
    "G. Ke et al., ‘LightGBM: A Highly Efficient Gradient Boosting Decision Tree’, in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hyper-parameter tuning with classic Bayesian Optimisation (TPE Sampler) but with manual adjustements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cleaned datasets\n",
    "dataset = pl.read_parquet(\"../data/clean/dataset.parquet\")\n",
    "sbmssn = pl.read_parquet(\"../data/clean/sbmssn.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_year                  float64\n",
       "child_date                 float64\n",
       "child_age                  float64\n",
       "child_enrolment_date       float64\n",
       "child_months_enrolment     float64\n",
       "                            ...   \n",
       "health_2                  category\n",
       "health_3                  category\n",
       "health_4                  category\n",
       "health_5                  category\n",
       "health_97                 category\n",
       "Length: 700, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.drop(['target']).to_pandas().dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training - test split\n",
    "train_data_x, test_data_x, train_data_y, test_data_y = train_test_split(\n",
    "    dataset.drop(['target']).to_pandas(),\n",
    "    dataset.get_column('target').to_pandas(),\n",
    "    train_size=0.9,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# training is splitted for tuning again\n",
    "eval_data_x, valid_data_x, eval_data_y, valid_data_y = train_test_split(\n",
    "    train_data_x,\n",
    "    train_data_y,\n",
    "    train_size=0.8,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# lgbm data format\n",
    "lgb_data_eval = lgb.Dataset(data=eval_data_x, label=eval_data_y)\n",
    "lgb_data_valid = lgb.Dataset(data=valid_data_x, label=valid_data_y, reference=lgb_data_eval)\n",
    "\n",
    "lgb_data_train = lgb.Dataset(data=train_data_x, label=train_data_y)\n",
    "lgb_data_test = lgb.Dataset(data=test_data_x, label=test_data_y, reference=lgb_data_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-23 09:06:57,555]\u001b[0m A new study created in memory with name: no-name-9d46a14d-0f4f-425c-a47d-ae5aaec8bb82\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a188f49e5b4e52aafd3f43d9534813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-04-23 09:06:59,884]\u001b[0m Trial 2 failed with parameters: {'cat_l2': 9.474948771293828, 'cat_smooth': 8.561186147719306} because of the following error: OSError('exception: access violation reading 0x0000000000000128').\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_7312\\308302607.py\", line 20, in objective\n",
      "    gbm = lgb.train(\n",
      "  File \"c:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\engine.py\", line 271, in train\n",
      "    booster = Booster(params=params, train_set=train_set)\n",
      "  File \"c:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\basic.py\", line 2605, in __init__\n",
      "    train_set.construct()\n",
      "  File \"c:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\basic.py\", line 1815, in construct\n",
      "    self._lazy_init(self.data, label=self.label,\n",
      "  File \"c:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\basic.py\", line 1557, in _lazy_init\n",
      "    self.set_label(label)\n",
      "  File \"c:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\basic.py\", line 2164, in set_label\n",
      "    self.set_field('label', label)\n",
      "  File \"c:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\basic.py\", line 1993, in set_field\n",
      "    _safe_call(_LIB.LGBM_DatasetSetField(\n",
      "OSError: exception: access violation reading 0x0000000000000128\n",
      "\u001b[33m[W 2023-04-23 09:06:59,998]\u001b[0m Trial 2 failed with value None.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 09:07:20,789]\u001b[0m Trial 1 finished with value: 9.567373776568774 and parameters: {'cat_l2': 6.792623611396756, 'cat_smooth': 8.569252302149545}. Best is trial 1 with value: 9.567373776568774.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 09:07:21,765]\u001b[0m Trial 3 finished with value: 9.579731609456353 and parameters: {'cat_l2': 14.979878896736514, 'cat_smooth': 6.918498596201321}. Best is trial 1 with value: 9.567373776568774.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 09:07:23,114]\u001b[0m Trial 0 finished with value: 9.51924658780816 and parameters: {'cat_l2': 5.489671684080044, 'cat_smooth': 13.746959791947695}. Best is trial 0 with value: 9.51924658780816.\u001b[0m\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "exception: access violation reading 0x0000000000000128",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 40\u001b[0m\n\u001b[0;32m     34\u001b[0m optuna\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39mset_verbosity(optuna\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39mINFO)\n\u001b[0;32m     36\u001b[0m study_bo \u001b[39m=\u001b[39m optuna\u001b[39m.\u001b[39mcreate_study(\n\u001b[0;32m     37\u001b[0m     sampler\u001b[39m=\u001b[39moptuna\u001b[39m.\u001b[39msamplers\u001b[39m.\u001b[39mTPESampler(seed\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m),\n\u001b[0;32m     38\u001b[0m     direction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mminimize\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     39\u001b[0m )\n\u001b[1;32m---> 40\u001b[0m study_bo\u001b[39m.\u001b[39;49moptimize(objective, n_trials\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, n_jobs\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m, show_progress_bar\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     42\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mNumber of finished trials:\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mlen\u001b[39m(study_bo\u001b[39m.\u001b[39mtrials))\n\u001b[0;32m     43\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mBest trial:\u001b[39m\u001b[39m'\u001b[39m, study_bo\u001b[39m.\u001b[39mbest_trial\u001b[39m.\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\optuna\\study\\study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimize\u001b[39m(\n\u001b[0;32m    322\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    330\u001b[0m     show_progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    331\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    333\u001b[0m \n\u001b[0;32m    334\u001b[0m \u001b[39m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[39m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     _optimize(\n\u001b[0;32m    426\u001b[0m         study\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m    427\u001b[0m         func\u001b[39m=\u001b[39;49mfunc,\n\u001b[0;32m    428\u001b[0m         n_trials\u001b[39m=\u001b[39;49mn_trials,\n\u001b[0;32m    429\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[0;32m    430\u001b[0m         n_jobs\u001b[39m=\u001b[39;49mn_jobs,\n\u001b[0;32m    431\u001b[0m         catch\u001b[39m=\u001b[39;49m\u001b[39mtuple\u001b[39;49m(catch) \u001b[39mif\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(catch, Iterable) \u001b[39melse\u001b[39;49;00m (catch,),\n\u001b[0;32m    432\u001b[0m         callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[0;32m    433\u001b[0m         gc_after_trial\u001b[39m=\u001b[39;49mgc_after_trial,\n\u001b[0;32m    434\u001b[0m         show_progress_bar\u001b[39m=\u001b[39;49mshow_progress_bar,\n\u001b[0;32m    435\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:103\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    101\u001b[0m                     \u001b[39m# Raise if exception occurred in executing the completed futures.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m                     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m completed:\n\u001b[1;32m--> 103\u001b[0m                         f\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    105\u001b[0m                 futures\u001b[39m.\u001b[39madd(\n\u001b[0;32m    106\u001b[0m                     executor\u001b[39m.\u001b[39msubmit(\n\u001b[0;32m    107\u001b[0m                         _optimize_sequential,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    118\u001b[0m                     )\n\u001b[0;32m    119\u001b[0m                 )\n\u001b[0;32m    120\u001b[0m \u001b[39mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfn(\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwargs)\n\u001b[0;32m     59\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m exc:\n\u001b[0;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfuture\u001b[39m.\u001b[39mset_exception(exc)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[39m=\u001b[39m _run_trial(study, func, catch)\n\u001b[0;32m    164\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[39m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[39m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[39m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[39m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[39massert\u001b[39;00m \u001b[39mFalse\u001b[39;00m, \u001b[39m\"\u001b[39m\u001b[39mShould not reach.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[39m.\u001b[39mstate \u001b[39m==\u001b[39m TrialState\u001b[39m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[39mand\u001b[39;00m func_err \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[39mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[39mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[39mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[39m.\u001b[39m_trial_id, study\u001b[39m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[39m=\u001b[39m func(trial)\n\u001b[0;32m    201\u001b[0m     \u001b[39mexcept\u001b[39;00m exceptions\u001b[39m.\u001b[39mTrialPruned \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[39m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[39m=\u001b[39m TrialState\u001b[39m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[9], line 20\u001b[0m, in \u001b[0;36mobjective\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m      3\u001b[0m params \u001b[39m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mobjective\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mregression\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      5\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmetric\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mseed\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m0\u001b[39m\n\u001b[0;32m     17\u001b[0m }\n\u001b[0;32m     19\u001b[0m \u001b[39m# FIXME: should I use here separate validation sets for early stopping and generalization estimate?\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m gbm \u001b[39m=\u001b[39m lgb\u001b[39m.\u001b[39;49mtrain(\n\u001b[0;32m     21\u001b[0m     params\u001b[39m=\u001b[39;49mparams,\n\u001b[0;32m     22\u001b[0m     num_boost_round\u001b[39m=\u001b[39;49m\u001b[39m5000\u001b[39;49m,\n\u001b[0;32m     23\u001b[0m     train_set\u001b[39m=\u001b[39;49mlgb_data_eval,\n\u001b[0;32m     24\u001b[0m     valid_sets\u001b[39m=\u001b[39;49m[lgb_data_valid],\n\u001b[0;32m     25\u001b[0m     callbacks\u001b[39m=\u001b[39;49m[lgb\u001b[39m.\u001b[39;49mearly_stopping(stopping_rounds\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)],\n\u001b[0;32m     26\u001b[0m )\n\u001b[0;32m     28\u001b[0m valid_data_pred \u001b[39m=\u001b[39m gbm\u001b[39m.\u001b[39mpredict(data\u001b[39m=\u001b[39mvalid_data_x)\n\u001b[0;32m     29\u001b[0m rmse \u001b[39m=\u001b[39m mean_squared_error(valid_data_y, valid_data_pred, squared\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\engine.py:271\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, train_set, num_boost_round, valid_sets, valid_names, fobj, feval, init_model, feature_name, categorical_feature, early_stopping_rounds, evals_result, verbose_eval, learning_rates, keep_training_booster, callbacks)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[39m# construct booster\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m     booster \u001b[39m=\u001b[39m Booster(params\u001b[39m=\u001b[39;49mparams, train_set\u001b[39m=\u001b[39;49mtrain_set)\n\u001b[0;32m    272\u001b[0m     \u001b[39mif\u001b[39;00m is_valid_contain_train:\n\u001b[0;32m    273\u001b[0m         booster\u001b[39m.\u001b[39mset_train_data_name(train_data_name)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\basic.py:2605\u001b[0m, in \u001b[0;36mBooster.__init__\u001b[1;34m(self, params, train_set, model_file, model_str, silent)\u001b[0m\n\u001b[0;32m   2598\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_network(\n\u001b[0;32m   2599\u001b[0m         machines\u001b[39m=\u001b[39mmachines,\n\u001b[0;32m   2600\u001b[0m         local_listen_port\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mlocal_listen_port\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   2601\u001b[0m         listen_time_out\u001b[39m=\u001b[39mparams\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtime_out\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m120\u001b[39m),\n\u001b[0;32m   2602\u001b[0m         num_machines\u001b[39m=\u001b[39mparams[\u001b[39m\"\u001b[39m\u001b[39mnum_machines\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   2603\u001b[0m     )\n\u001b[0;32m   2604\u001b[0m \u001b[39m# construct booster object\u001b[39;00m\n\u001b[1;32m-> 2605\u001b[0m train_set\u001b[39m.\u001b[39;49mconstruct()\n\u001b[0;32m   2606\u001b[0m \u001b[39m# copy the parameters from train_set\u001b[39;00m\n\u001b[0;32m   2607\u001b[0m params\u001b[39m.\u001b[39mupdate(train_set\u001b[39m.\u001b[39mget_params())\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\basic.py:1815\u001b[0m, in \u001b[0;36mDataset.construct\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1812\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_init_score_by_predictor(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_predictor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata, used_indices)\n\u001b[0;32m   1813\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1814\u001b[0m     \u001b[39m# create train\u001b[39;00m\n\u001b[1;32m-> 1815\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_lazy_init(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata, label\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel,\n\u001b[0;32m   1816\u001b[0m                     weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, group\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroup,\n\u001b[0;32m   1817\u001b[0m                     init_score\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minit_score, predictor\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predictor,\n\u001b[0;32m   1818\u001b[0m                     silent\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msilent, feature_name\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_name,\n\u001b[0;32m   1819\u001b[0m                     categorical_feature\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategorical_feature, params\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[0;32m   1820\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfree_raw_data:\n\u001b[0;32m   1821\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\basic.py:1557\u001b[0m, in \u001b[0;36mDataset._lazy_init\u001b[1;34m(self, data, label, reference, weight, group, init_score, predictor, silent, feature_name, categorical_feature, params)\u001b[0m\n\u001b[0;32m   1555\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mCannot initialize Dataset from \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(data)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m label \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1557\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_label(label)\n\u001b[0;32m   1558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_label() \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1559\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mLabel should not be None\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\basic.py:2164\u001b[0m, in \u001b[0;36mDataset.set_label\u001b[1;34m(self, label)\u001b[0m\n\u001b[0;32m   2162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2163\u001b[0m     label \u001b[39m=\u001b[39m list_to_1d_numpy(_label_from_pandas(label), name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m-> 2164\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_field(\u001b[39m'\u001b[39;49m\u001b[39mlabel\u001b[39;49m\u001b[39m'\u001b[39;49m, label)\n\u001b[0;32m   2165\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_field(\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# original values can be modified at cpp side\u001b[39;00m\n\u001b[0;32m   2166\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\lightgbm\\basic.py:1993\u001b[0m, in \u001b[0;36mDataset.set_field\u001b[1;34m(self, field_name, data)\u001b[0m\n\u001b[0;32m   1991\u001b[0m \u001b[39mif\u001b[39;00m type_data \u001b[39m!=\u001b[39m FIELD_TYPE_MAPPER[field_name]:\n\u001b[0;32m   1992\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInput type error for set_field\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 1993\u001b[0m _safe_call(_LIB\u001b[39m.\u001b[39;49mLGBM_DatasetSetField(\n\u001b[0;32m   1994\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhandle,\n\u001b[0;32m   1995\u001b[0m     c_str(field_name),\n\u001b[0;32m   1996\u001b[0m     ptr_data,\n\u001b[0;32m   1997\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(\u001b[39mlen\u001b[39;49m(data)),\n\u001b[0;32m   1998\u001b[0m     ctypes\u001b[39m.\u001b[39;49mc_int(type_data)))\n\u001b[0;32m   1999\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mversion \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   2000\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[1;31mOSError\u001b[0m: exception: access violation reading 0x0000000000000128"
     ]
    }
   ],
   "source": [
    "# optuna hyper-parameter optimization with TPE sampler\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.01,\n",
    "        # 'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-4, 100.0),\n",
    "        # 'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-4, 100.0),\n",
    "        # 'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n",
    "        # 'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "        # 'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "        'cat_l2': trial.suggest_uniform('cat_l2', 5, 15),\n",
    "        'cat_smooth': trial.suggest_uniform('cat_smooth', 5, 15),\n",
    "        'verbosity': -1,\n",
    "        'seed': 0\n",
    "    }\n",
    "\n",
    "    # FIXME: should I use here separate validation sets for early stopping and generalization estimate?\n",
    "    gbm = lgb.train(\n",
    "        params=params,\n",
    "        num_boost_round=5000,\n",
    "        train_set=lgb_data_eval,\n",
    "        valid_sets=[lgb_data_valid],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)],\n",
    "    )\n",
    "\n",
    "    valid_data_pred = gbm.predict(data=valid_data_x)\n",
    "    rmse = mean_squared_error(valid_data_y, valid_data_pred, squared=False)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "study_bo = optuna.create_study(\n",
    "    sampler=optuna.samplers.TPESampler(seed=0),\n",
    "    direction='minimize'\n",
    ")\n",
    "study_bo.optimize(objective, n_trials=100, n_jobs=4, show_progress_bar=True)\n",
    "\n",
    "print('Number of finished trials:', len(study_bo.trials))\n",
    "print('Best trial:', study_bo.best_trial.params)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = optuna.visualization.plot_optimization_history(study_bo)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = optuna.visualization.plot_intermediate_values(study_hb)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = optuna.visualization.plot_contour(study_hb)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.031391 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 13306\n",
      "[LightGBM] [Info] Number of data points in the train set: 7726, number of used features: 698\n",
      "[LightGBM] [Info] Start training from score 48.700467\n"
     ]
    }
   ],
   "source": [
    "# refit model on complete training dataset and check prediction error\n",
    "lgb_dataset = lgb.Dataset(\n",
    "    data=train_data_x,\n",
    "    label=train_data_y\n",
    ")\n",
    "\n",
    "bst = lgb.train(\n",
    "    params=study_bo.best_params, \n",
    "    train_set=lgb_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 9.731916\n"
     ]
    }
   ],
   "source": [
    "test_data_preds = bst.predict(data=test_data_x)\n",
    "test_rmse = mean_squared_error(test_data_y, test_data_preds, squared=False)\n",
    "\n",
    "print('Score', round(test_rmse, ndigits=6))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Final model fit with tuned params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a final model to complete training set and predict submission data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load model\n",
    "# bst.save_model('model.txt')\n",
    "# bst = lgb.Booster(model_file='model.txt')  # init model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
