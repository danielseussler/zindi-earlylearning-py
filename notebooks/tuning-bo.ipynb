{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "import ConfigSpace as CS\n",
    "from dehb import DEHB\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import time\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 679)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>child_id</th><th>data_year</th><th>child_date</th><th>child_age</th><th>child_enrolment_date</th><th>child_months_enrolment</th><th>child_grant</th><th>child_years_in_programme</th><th>child_height</th><th>child_observe_attentive</th><th>child_observe_concentrated</th><th>child_observe_diligent</th><th>child_observe_interested</th><th>child_observe_total</th><th>child_gender</th><th>child_dob</th><th>child_zha</th><th>child_stunted</th><th>child_attends</th><th>child_attendance</th><th>child_languages</th><th>child_age_group</th><th>id_mn_best</th><th>prov_best</th><th>id_dc_best</th><th>dc_best</th><th>mn_best</th><th>ward_best</th><th>id_enumerator</th><th>id_facility</th><th>pqa_date</th><th>pqa_class_age</th><th>pqa_class_age_1</th><th>pqa_class_age_2</th><th>pqa_class_age_3</th><th>pqa_class_age_4</th><th>pqa_class_age_5</th><th>&hellip;</th><th>positionother</th><th>positionotherreason</th><th>sef_ind</th><th>language_match</th><th>elp_ind</th><th>gps_ind</th><th>pre_covid</th><th>ses_proxy</th><th>quintile_used</th><th>id_facility_n</th><th>id_ward_n</th><th>id_mn_n</th><th>id_dc_n</th><th>id_prov_n</th><th>language_assessment_w2</th><th>ses_cat</th><th>obs_lighting_1</th><th>obs_lighting_2</th><th>obs_lighting_3</th><th>obs_lighting_4</th><th>obs_lighting_5</th><th>obs_lighting_6</th><th>obs_lighting_8</th><th>obs_cooking_1</th><th>obs_cooking_2</th><th>obs_cooking_3</th><th>obs_cooking_4</th><th>obs_cooking_5</th><th>obs_cooking_6</th><th>obs_heating_1</th><th>obs_heating_2</th><th>obs_heating_3</th><th>obs_heating_4</th><th>obs_heating_5</th><th>obs_heating_6</th><th>obs_heating_7</th><th>target</th></tr><tr><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>str</td><td>&hellip;</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;ID_SYSJ2FM0D&quot;</td><td>2022.0</td><td>&quot;2022-02-03&quot;</td><td>59.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;Sometimes&quot;</td><td>&quot;Sometimes&quot;</td><td>&quot;Sometimes&quot;</td><td>&quot;Sometimes&quot;</td><td>4.0</td><td>&quot;Female&quot;</td><td>&quot;2017-02-06&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&quot;50-59 months&quot;</td><td>&quot;GT421&quot;</td><td>&quot;GAUTENG&quot;</td><td>&quot;DC42&quot;</td><td>&quot;SEDIBENG&quot;</td><td>&quot;EMFULENI&quot;</td><td>39.0</td><td>20005.0</td><td>761.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>&quot;Yes&quot;</td><td>1.0</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Post COVID&quot;</td><td>2.0</td><td>&quot;Yes&quot;</td><td>7.0</td><td>14.0</td><td>107.0</td><td>134.0</td><td>1051.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>51.5</td></tr><tr><td>&quot;ID_J5BTFOZR3&quot;</td><td>2019.0</td><td>null</td><td>60.163933</td><td>null</td><td>null</td><td>null</td><td>&quot;1st year in th…</td><td>103.0</td><td>&quot;Sometimes&quot;</td><td>&quot;Almost never&quot;</td><td>&quot;Sometimes&quot;</td><td>&quot;Often&quot;</td><td>4.0</td><td>&quot;Female&quot;</td><td>null</td><td>-1.356791</td><td>&quot;Normal&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;60-69 months&quot;</td><td>null</td><td>&quot;KWAZULU-NATAL&quot;</td><td>&quot;DC22&quot;</td><td>&quot;UMGUNGUNDLOVU&quot;</td><td>null</td><td>null</td><td>null</td><td>458.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>&quot;Yes&quot;</td><td>1.0</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;Pre-COVID&quot;</td><td>4.0</td><td>&quot;Yes&quot;</td><td>24.0</td><td>null</td><td>null</td><td>367.0</td><td>1832.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>55.869999</td></tr><tr><td>&quot;ID_R00SN7AUD&quot;</td><td>2022.0</td><td>&quot;2022-03-11&quot;</td><td>69.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>108.400002</td><td>&quot;Often&quot;</td><td>&quot;Often&quot;</td><td>&quot;Sometimes&quot;</td><td>&quot;Often&quot;</td><td>7.0</td><td>&quot;Male&quot;</td><td>&quot;2016-05-24&quot;</td><td>-1.250863</td><td>&quot;Normal&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;60-69 months&quot;</td><td>&quot;CPT&quot;</td><td>&quot;WESTERN CAPE&quot;</td><td>&quot;CPT&quot;</td><td>&quot;CITY OF CAPE T…</td><td>&quot;CITY OF CAPE T…</td><td>85.0</td><td>20001.0</td><td>925.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>&quot;No&quot;</td><td>1.0</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Post COVID&quot;</td><td>1.0</td><td>&quot;No&quot;</td><td>8.0</td><td>24.0</td><td>1448.0</td><td>1448.0</td><td>3214.0</td><td>null</td><td>&quot;R0-110&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>47.52</td></tr><tr><td>&quot;ID_BSSK60PAZ&quot;</td><td>2021.0</td><td>&quot;2021-10-13&quot;</td><td>53.0</td><td>&quot;2020-01-15&quot;</td><td>20.0</td><td>&quot;No&quot;</td><td>&quot;1st year in th…</td><td>98.099998</td><td>&quot;Almost always&quot;</td><td>&quot;Almost always&quot;</td><td>&quot;Sometimes&quot;</td><td>&quot;Often&quot;</td><td>9.0</td><td>&quot;Male&quot;</td><td>&quot;2017-05-08&quot;</td><td>-1.830364</td><td>&quot;Normal&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;50-59 months&quot;</td><td>&quot;WC025&quot;</td><td>&quot;WESTERN CAPE&quot;</td><td>&quot;DC2&quot;</td><td>&quot;CAPE WINELANDS…</td><td>&quot;BREEDE VALLEY&quot;</td><td>18.0</td><td>2689.0</td><td>308.0</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>&hellip;</td><td>null</td><td>null</td><td>&quot;Yes&quot;</td><td>1.0</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Post COVID&quot;</td><td>3.0</td><td>&quot;No&quot;</td><td>4.0</td><td>22.0</td><td>76.0</td><td>629.0</td><td>3214.0</td><td>null</td><td>&quot;R291-750&quot;</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>58.599998</td></tr><tr><td>&quot;ID_IZTY6TC4D&quot;</td><td>2021.0</td><td>&quot;2021-10-13&quot;</td><td>57.0</td><td>&quot;2021-10-13&quot;</td><td>0.0</td><td>null</td><td>&quot;2nd year in pr…</td><td>114.0</td><td>&quot;Almost always&quot;</td><td>&quot;Almost always&quot;</td><td>&quot;Almost always&quot;</td><td>&quot;Almost always&quot;</td><td>12.0</td><td>&quot;Female&quot;</td><td>&quot;2016-12-19&quot;</td><td>1.3292638</td><td>&quot;Normal&quot;</td><td>null</td><td>null</td><td>null</td><td>&quot;50-59 months&quot;</td><td>&quot;KZN293&quot;</td><td>&quot;KWAZULU-NATAL&quot;</td><td>&quot;DC29&quot;</td><td>&quot;ILEMBE&quot;</td><td>&quot;NDWEDWE&quot;</td><td>10.0</td><td>542.0</td><td>1749.0</td><td>&quot;2021-10-29&quot;</td><td>&quot;4 5&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;No&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&hellip;</td><td>null</td><td>null</td><td>&quot;Yes&quot;</td><td>1.0</td><td>&quot;Yes&quot;</td><td>&quot;Yes&quot;</td><td>&quot;Post COVID&quot;</td><td>1.0</td><td>&quot;No&quot;</td><td>1.0</td><td>30.0</td><td>71.0</td><td>315.0</td><td>1832.0</td><td>null</td><td>&quot;R0-110&quot;</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>1.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>0.0</td><td>76.599998</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 679)\n",
       "┌───────────┬─────────┬──────────┬───────────┬───┬────────────┬────────────┬────────────┬──────────┐\n",
       "│ child_id  ┆ data_ye ┆ child_da ┆ child_age ┆ … ┆ obs_heatin ┆ obs_heatin ┆ obs_heatin ┆ target   │\n",
       "│ ---       ┆ ar      ┆ te       ┆ ---       ┆   ┆ g_5        ┆ g_6        ┆ g_7        ┆ ---      │\n",
       "│ str       ┆ ---     ┆ ---      ┆ f64       ┆   ┆ ---        ┆ ---        ┆ ---        ┆ f64      │\n",
       "│           ┆ f64     ┆ str      ┆           ┆   ┆ f64        ┆ f64        ┆ f64        ┆          │\n",
       "╞═══════════╪═════════╪══════════╪═══════════╪═══╪════════════╪════════════╪════════════╪══════════╡\n",
       "│ ID_SYSJ2F ┆ 2022.0  ┆ 2022-02- ┆ 59.0      ┆ … ┆ null       ┆ null       ┆ null       ┆ 51.5     │\n",
       "│ M0D       ┆         ┆ 03       ┆           ┆   ┆            ┆            ┆            ┆          │\n",
       "│ ID_J5BTFO ┆ 2019.0  ┆ null     ┆ 60.163933 ┆ … ┆ null       ┆ null       ┆ null       ┆ 55.86999 │\n",
       "│ ZR3       ┆         ┆          ┆           ┆   ┆            ┆            ┆            ┆ 9        │\n",
       "│ ID_R00SN7 ┆ 2022.0  ┆ 2022-03- ┆ 69.0      ┆ … ┆ null       ┆ null       ┆ null       ┆ 47.52    │\n",
       "│ AUD       ┆         ┆ 11       ┆           ┆   ┆            ┆            ┆            ┆          │\n",
       "│ ID_BSSK60 ┆ 2021.0  ┆ 2021-10- ┆ 53.0      ┆ … ┆ null       ┆ null       ┆ null       ┆ 58.59999 │\n",
       "│ PAZ       ┆         ┆ 13       ┆           ┆   ┆            ┆            ┆            ┆ 8        │\n",
       "│ ID_IZTY6T ┆ 2021.0  ┆ 2021-10- ┆ 57.0      ┆ … ┆ 0.0        ┆ 0.0        ┆ 0.0        ┆ 76.59999 │\n",
       "│ C4D       ┆         ┆ 13       ┆           ┆   ┆            ┆            ┆            ┆ 8        │\n",
       "└───────────┴─────────┴──────────┴───────────┴───┴────────────┴────────────┴────────────┴──────────┘"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataset = pl.read_csv('../data/Train.csv', ignore_errors=True)\n",
    "sbmssn = pl.read_csv('../data/Test.csv', ignore_errors=True)\n",
    "\n",
    "dataset.head(n=5)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Some basic data cleaning and feature pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some columns\n",
    "deselect_col = [\n",
    "    'child_id',\n",
    "    'obs_materials', 'obs_handwashing', 'obs_area', 'obs_toilet',\n",
    "    'obs_equipment', 'obs_safety', 'obs_hazard', 'pqa_class_age',\n",
    "    'pra_groupings', 'pra_plans', 'obs_access_disability', 'pra_cohort',\n",
    "    'pra_plan_4yrs', 'pra_plan_5yrs', 'pra_plan_4yrsother', 'pra_plan_5yrsother',\n",
    "    'pra_qualification', 'pra_ncf_trainer', 'pra_training', 'pri_language',\n",
    "    'pri_meal', 'pri_money',  'pri_funding_salary', 'pri_expenseother',\n",
    "    'pri_clinic_travel', 'pri_covid_awareness', 'pri_covid_precautions',\n",
    "    'pri_food_type', 'pri_records', 'pri_support_provider', \n",
    "    'pri_qualification', 'pri_email_network_forum', \n",
    "    'pri_reason_register_year',  \n",
    "     \n",
    "    #'pri_support_providerother', 'pri_languageother', 'pri_moneyother',\n",
    "    #'pri_dsd_unregistered_other', 'pri_dsd_conditional_other', 'pri_landother', 'pri_fundingother', 'pri_facilitiesother',\n",
    "    #'pri_fees_exceptions_other', 'pri_staff_changes_reasonsother', 'hle_ecd_other', 'obs_water_running_none', 'obs_waterother'\n",
    "]\n",
    "\n",
    "dataset = dataset.select(pl.all().exclude(deselect_col))\n",
    "sbmssn = sbmssn.select(pl.all().exclude(list(set(deselect_col) - set(['child_id']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date columns we are going to recode\n",
    "date_col = ['child_date', 'child_enrolment_date', 'child_dob', 'pqa_date', 'pra_date', 'pri_date', 'obs_date']\n",
    "\n",
    "for cn in date_col: \n",
    "    dataset = dataset.with_columns(pl.col(cn).str.strptime(pl.Date, '%Y-%m-%d'))\n",
    "    sbmssn = sbmssn.with_columns(pl.col(cn).str.strptime(pl.Date, '%Y-%m-%d'))\n",
    "\n",
    "for cn in date_col: \n",
    "    dataset = dataset.with_columns([\n",
    "        pl.col(cn).dt.year().alias(cn + '_year').cast(pl.Utf8).cast(pl.Categorical), \n",
    "        pl.col(cn).dt.month().alias(cn + '_month').cast(pl.Utf8).cast(pl.Categorical), \n",
    "        pl.col(cn).dt.quarter().alias(cn + '_quarter').cast(pl.Utf8).cast(pl.Categorical),\n",
    "        pl.col(cn).dt.week().alias(cn + '_week').cast(pl.Utf8).cast(pl.Categorical), \n",
    "        pl.col(cn).cast(pl.Int32)\n",
    "    ])\n",
    "\n",
    "    sbmssn = sbmssn.with_columns([\n",
    "        pl.col(cn).dt.year().alias(cn + '_year').cast(pl.Utf8).cast(pl.Categorical), \n",
    "        pl.col(cn).dt.month().alias(cn + '_month').cast(pl.Utf8).cast(pl.Categorical), \n",
    "        pl.col(cn).dt.quarter().alias(cn + '_quarter').cast(pl.Utf8).cast(pl.Categorical),\n",
    "        pl.col(cn).dt.week().alias(cn + '_week').cast(pl.Utf8).cast(pl.Categorical), \n",
    "        pl.col(cn).cast(pl.Int32)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert this to categorical\n",
    "dataset = dataset.with_columns([\n",
    "    pl.col('language_match').map_dict({0: 'No', 1: 'Yes'}, default=None).cast(pl.Categorical)\n",
    "])\n",
    "\n",
    "sbmssn = sbmssn.with_columns([\n",
    "    pl.col('language_match').map_dict({0: 'No', 1: 'Yes'}, default=None).cast(pl.Categorical)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Setswana', 'isiXhosa + Xitsonga +', 'English + Setswana +', 'English + isiZulu +', 'Tshivenda', 'English + Sesotho se Leboa (Sepedi) +', None, 'English + isiXhosa +', 'isiZulu', 'Sesotho se Leboa (Sepedi)', 'English + Xitsonga +', 'Afrikaans', 'English + isiNdebele +', 'English', 'isiZulu + isiXhosa +', 'Sesotho', 'English + isiZulu + isiXhosa', 'isiXhosa', 'English + Afrikaans +', 'Afrikaans + Setswana +']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['child_languages'].unique().to_list(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract child languages here\n",
    "child_langs = ['Afrikaans', 'English', 'isiNdebele', 'isiXhosa',\n",
    "               'isiZulu', 'Xitsonga', 'Setswana', 'Sesotho', 'Sepedi']\n",
    "\n",
    "for ln in child_langs:\n",
    "    dataset = dataset.with_columns([\n",
    "        pl.col('child_languages').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'child_languages_{ln}')\n",
    "    ])\n",
    "\n",
    "    sbmssn = sbmssn.with_columns([\n",
    "        pl.col('child_languages').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'child_languages_{ln}')\n",
    "    ])\n",
    "\n",
    "dataset = dataset.drop('child_languages')\n",
    "sbmssn = sbmssn.drop('child_languages')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# FIXME count languages a child speaks\n",
    "dataset = dataset.with_columns([\n",
    "    pl.sum(\n",
    "        pl.col('^child_languages.*$').\n",
    "        cast(pl.Utf8).\n",
    "        map_dict({'No': 0, 'Yes': 1}, default=None, return_dtype=pl.Int64)\n",
    "    ).alias(\"asbdkjasf\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English + isiZulu +', 'Afrikaans', 'isiZulu', 'Sesotho', 'English + Afrikaans +', 'Setswana', 'English', 'isiXhosa', 'English + isiXhosa +', 'Afrikaans + isiZulu +', None, 'English + Afrikaans + isiXhosa']\n"
     ]
    }
   ],
   "source": [
    "print(dataset['pri_languages'].unique().to_list(), sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract pri_languages here\n",
    "pri_langs = ['Afrikaans', 'English', 'isiXhosa', 'isiZulu', 'Setswana', 'Sesotho']\n",
    "\n",
    "for ln in pri_langs:\n",
    "    dataset = dataset.with_columns([\n",
    "        pl.col('pri_languages').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'pri_languages_{ln}')\n",
    "    ])\n",
    "\n",
    "    sbmssn = sbmssn.with_columns([\n",
    "        pl.col('pri_languages').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'pri_languages_{ln}')\n",
    "    ])\n",
    "\n",
    "dataset = dataset.drop('pri_languages')\n",
    "sbmssn = sbmssn.drop('pri_languages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the gps column and add polar coordinates\n",
    "dataset = dataset.with_columns(\n",
    "    pl.col('gps').str.split(' ').\\\n",
    "    arr.to_struct(n_field_strategy='max_width').\\\n",
    "    struct.rename_fields(['gps_lat', 'gps_lon'])).\\\n",
    "    unnest('gps')\n",
    "\n",
    "sbmssn = sbmssn.with_columns(\n",
    "    pl.col('gps').str.split(' ').\\\n",
    "    arr.to_struct(n_field_strategy='max_width').\\\n",
    "    struct.rename_fields(['gps_lat', 'gps_lon'])).\\\n",
    "    unnest('gps')\n",
    "\n",
    "# cast to numeric\n",
    "for cn in ['gps_lat', 'gps_lon']:\n",
    "    dataset = dataset.with_columns(pl.col(cn).cast(pl.Float64))\n",
    "    sbmssn = sbmssn.with_columns(pl.col(cn).cast(pl.Float64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerics I code as categories\n",
    "numcat_columns = ['id_ward', 'id_enumerator', 'id_facility', 'id_team']\n",
    "\n",
    "for cn in numcat_columns: \n",
    "    dataset = dataset.with_columns(pl.col(cn).cast(pl.Int64).cast(pl.Utf8).cast(pl.Categorical))\n",
    "    sbmssn = sbmssn.with_columns(pl.col(cn).cast(pl.Int64).cast(pl.Utf8).cast(pl.Categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace values with NA's in train.data\n",
    "dataset = dataset.with_columns([\n",
    "    pl.when(pl.col(pl.Utf8).str.contains('Do Not Know|Missing|Don\\'t know|Refuse|DON\\'T KNOW|DON\\'T KNOW.|DON\\'T KNOW THE REASON FOR CONDITIONAL'))\n",
    "    .then(None)\n",
    "    .otherwise(pl.col(pl.Utf8))\n",
    "    .keep_name()\n",
    "])\n",
    "\n",
    "sbmssn = sbmssn.with_columns([\n",
    "    pl.when(pl.col(pl.Utf8).str.contains('Do Not Know|Missing|Don\\'t know|Refuse|DON\\'T KNOW|DON\\'T KNOW.|DON\\'T KNOW THE REASON FOR CONDITIONAL'))\n",
    "    .then(None)\n",
    "    .otherwise(pl.col(pl.Utf8))\n",
    "    .keep_name()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column pri_network_type has different data types between the two data frames.\n",
      "Column pri_staff_changes_reasons has different data types between the two data frames.\n",
      "Column pri_food_donor has different data types between the two data frames.\n"
     ]
    }
   ],
   "source": [
    "# check where dataset and sbmssn diasgree on data types\n",
    "for cn in dataset.select(pl.exclude('target')).columns:\n",
    "    if dataset[cn].dtype != sbmssn[cn].dtype:\n",
    "        print(f'Column {cn} has different data types between the two data frames.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix those columns\n",
    "for answer in [1, 2, 3, 97]:\n",
    "    dataset = dataset.with_columns([\n",
    "        pl.col('pri_network_type').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'pri_network_type_{answer}')\n",
    "    ])\n",
    "\n",
    "    sbmssn = sbmssn.with_columns([\n",
    "        pl.col('pri_network_type').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'pri_network_type_{answer}')\n",
    "    ])\n",
    "\n",
    "for answer in [1, 2, 3, 4, 5, 6, 97]:\n",
    "    dataset = dataset.with_columns([\n",
    "        pl.col('pri_staff_changes_reasons').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'pri_staff_changes_reasons_{answer}')\n",
    "    ])\n",
    "\n",
    "    sbmssn = sbmssn.with_columns([\n",
    "        pl.col('pri_staff_changes_reasons').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'pri_staff_changes_reasons_{answer}')\n",
    "    ])\n",
    "\n",
    "for answer in [1, 2, 3, 4, 97]:\n",
    "    dataset = dataset.with_columns([\n",
    "        pl.col('pri_food_donor').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'pri_food_donor_{answer}')\n",
    "    ])\n",
    "\n",
    "    sbmssn = sbmssn.with_columns([\n",
    "        pl.col('pri_food_donor').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'pri_food_donor_{answer}')\n",
    "    ])\n",
    "\n",
    "# drop the splitted columns\n",
    "dataset = dataset.drop(\n",
    "    ['pri_network_type', 'pri_staff_changes_reasons', 'pri_food_donor'])\n",
    "sbmssn = sbmssn.drop(\n",
    "    ['pri_network_type', 'pri_staff_changes_reasons', 'pri_food_donor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical encoding of some other survey responses\n",
    "for answer in [1, 2, 3, 4, 5, 97]:\n",
    "    dataset = dataset.with_columns([\n",
    "        pl.col('health').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'health_{answer}')\n",
    "    ])\n",
    "\n",
    "    sbmssn = sbmssn.with_columns([\n",
    "        pl.col('health').\n",
    "        str.contains(ln).\n",
    "        map_dict({0: 'No', 1: 'Yes'}, default=None).\n",
    "        cast(pl.Categorical).\n",
    "        alias(f'health_{answer}')\n",
    "    ])\n",
    "\n",
    "# drop the splitted columns\n",
    "dataset = dataset.drop('health')\n",
    "sbmssn = sbmssn.drop('health')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other strings to categories\n",
    "for cn in dataset.select(pl.col(pl.Utf8)).columns: \n",
    "    dataset = dataset.with_columns(pl.col(cn).cast(pl.Categorical))\n",
    "    sbmssn = sbmssn.with_columns(pl.col(cn).cast(pl.Categorical))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training - test split\n",
    "train_data_x, test_data_x, train_data_y, test_data_y = train_test_split(\n",
    "    dataset.drop(['target']).to_pandas(),\n",
    "    dataset.get_column('target').to_pandas(),\n",
    "    train_size=0.9,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# training is splitted for tuning again\n",
    "eval_data_x, valid_data_x, eval_data_y, valid_data_y = train_test_split(\n",
    "    train_data_x,\n",
    "    train_data_y,\n",
    "    train_size=0.8,\n",
    "    random_state=0\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Strategy: Gradient Boosted Trees, specifically 'LightGBM'.\n",
    "\n",
    "G. Ke et al., ‘LightGBM: A Highly Efficient Gradient Boosting Decision Tree’, in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2017. [Online]. Available: https://proceedings.neurips.cc/paper_files/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hyper-parameter tuning with classic Bayesian Optimisation (TPE Sampler)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-23 07:03:02,034]\u001b[0m A new study created in memory with name: no-name-822c223a-4c02-4be2-b706-4de75f832652\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f90fd40a8fa470bae7cd36c7d372995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-23 07:03:23,130]\u001b[0m Trial 3 finished with value: 9.516950154345658 and parameters: {'num_leaves': 33, 'feature_fraction': 0.67522818396871, 'bagging_fraction': 0.7946888182046625, 'cat_l2': 11.712726281338782, 'cat_smooth': 5.8864481844643315}. Best is trial 3 with value: 9.516950154345658.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:03:45,144]\u001b[0m Trial 1 finished with value: 9.536714913662676 and parameters: {'num_leaves': 121, 'feature_fraction': 0.963294595018171, 'bagging_fraction': 0.6511792538367593, 'cat_l2': 5.74268910951174, 'cat_smooth': 6.377937785515004}. Best is trial 3 with value: 9.516950154345658.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:03:55,001]\u001b[0m Trial 0 finished with value: 9.5115203874006 and parameters: {'num_leaves': 152, 'feature_fraction': 0.7564543008932688, 'bagging_fraction': 0.8433698672097105, 'cat_l2': 12.976032920349365, 'cat_smooth': 12.661385701411858}. Best is trial 0 with value: 9.5115203874006.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:04:01,253]\u001b[0m Trial 4 finished with value: 9.594124145383308 and parameters: {'num_leaves': 130, 'feature_fraction': 0.9459768008212506, 'bagging_fraction': 0.6399308204201567, 'cat_l2': 6.884660058257324, 'cat_smooth': 5.656844920716978}. Best is trial 0 with value: 9.5115203874006.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:04:05,241]\u001b[0m Trial 2 finished with value: 9.596369907471832 and parameters: {'num_leaves': 241, 'feature_fraction': 0.8448162366444882, 'bagging_fraction': 0.7161566084415969, 'cat_l2': 5.7773758947892855, 'cat_smooth': 9.345881005841306}. Best is trial 0 with value: 9.5115203874006.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:04:18,927]\u001b[0m Trial 7 finished with value: 9.54964084013175 and parameters: {'num_leaves': 18, 'feature_fraction': 0.784335623267591, 'bagging_fraction': 0.9476957360097114, 'cat_l2': 8.016701340209675, 'cat_smooth': 6.154449080691861}. Best is trial 0 with value: 9.5115203874006.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:04:22,355]\u001b[0m Trial 6 finished with value: 9.518168764059316 and parameters: {'num_leaves': 40, 'feature_fraction': 0.9344521254731752, 'bagging_fraction': 0.79361687681697, 'cat_l2': 5.548187526979955, 'cat_smooth': 14.589010496024118}. Best is trial 0 with value: 9.5115203874006.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:04:24,761]\u001b[0m Trial 8 finished with value: 9.601625656253344 and parameters: {'num_leaves': 26, 'feature_fraction': 0.9728262767467234, 'bagging_fraction': 0.7235150238215632, 'cat_l2': 14.682744066129777, 'cat_smooth': 5.832383810893887}. Best is trial 0 with value: 9.5115203874006.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:04:26,250]\u001b[0m Trial 5 finished with value: 9.446461878059988 and parameters: {'num_leaves': 92, 'feature_fraction': 0.7078181747337616, 'bagging_fraction': 0.8445852456207387, 'cat_l2': 5.9110520212070945, 'cat_smooth': 11.606357306267455}. Best is trial 5 with value: 9.446461878059988.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:04:54,756]\u001b[0m Trial 9 finished with value: 9.563714016142393 and parameters: {'num_leaves': 78, 'feature_fraction': 0.9999421451346475, 'bagging_fraction': 0.8066820496411814, 'cat_l2': 10.03481182235405, 'cat_smooth': 5.506457057606679}. Best is trial 5 with value: 9.446461878059988.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:05:38,729]\u001b[0m Trial 12 finished with value: 9.575455400860804 and parameters: {'num_leaves': 207, 'feature_fraction': 0.8466339557983882, 'bagging_fraction': 0.8466670390463804, 'cat_l2': 11.361843450755265, 'cat_smooth': 5.46891371139324}. Best is trial 5 with value: 9.446461878059988.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:05:46,371]\u001b[0m Trial 10 finished with value: 9.504068409349175 and parameters: {'num_leaves': 146, 'feature_fraction': 0.9087445265550952, 'bagging_fraction': 0.7642588048068581, 'cat_l2': 13.749980250186368, 'cat_smooth': 13.281629456922907}. Best is trial 5 with value: 9.446461878059988.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:05:49,382]\u001b[0m Trial 11 finished with value: 9.568608011959627 and parameters: {'num_leaves': 192, 'feature_fraction': 0.8841098447744034, 'bagging_fraction': 0.9193501799661148, 'cat_l2': 7.02367568033017, 'cat_smooth': 9.38459145956753}. Best is trial 5 with value: 9.446461878059988.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:06:19,279]\u001b[0m Trial 13 finished with value: 9.495001330673865 and parameters: {'num_leaves': 194, 'feature_fraction': 0.6207092834375647, 'bagging_fraction': 0.9869093630772223, 'cat_l2': 8.797747100206552, 'cat_smooth': 10.843139939623596}. Best is trial 5 with value: 9.446461878059988.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:06:26,032]\u001b[0m Trial 16 finished with value: 9.424038342422572 and parameters: {'num_leaves': 84, 'feature_fraction': 0.6154357434051942, 'bagging_fraction': 0.9918346544375656, 'cat_l2': 8.499606330917644, 'cat_smooth': 12.146340382973111}. Best is trial 16 with value: 9.424038342422572.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:06:29,288]\u001b[0m Trial 15 finished with value: 9.44948227310275 and parameters: {'num_leaves': 97, 'feature_fraction': 0.6002464823907511, 'bagging_fraction': 0.9083638961101514, 'cat_l2': 8.436531226032875, 'cat_smooth': 12.051393449197189}. Best is trial 16 with value: 9.424038342422572.\u001b[0m\n",
      "\u001b[32m[I 2023-04-23 07:06:42,164]\u001b[0m Trial 14 finished with value: 9.536042729740512 and parameters: {'num_leaves': 176, 'feature_fraction': 0.6725469863294844, 'bagging_fraction': 0.9069655662179783, 'cat_l2': 8.300750411861875, 'cat_smooth': 12.508356268594829}. Best is trial 16 with value: 9.424038342422572.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# optuna hyper-parameter optimization with TPE sampler\n",
    "lgb_data_eval = lgb.Dataset(data=eval_data_x, label=eval_data_y)\n",
    "lgb_data_valid = lgb.Dataset(data=valid_data_x, label=valid_data_y, reference=lgb_data_eval)\n",
    "lgb_data_test = lgb.Dataset(data=test_data_x, label=test_data_y, reference=lgb_data_eval)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.01, \n",
    "        # 'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-4, 100.0),\n",
    "        # 'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-4, 100.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n",
    "        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "        'cat_l2': trial.suggest_uniform('cat_l2', 5, 15),\n",
    "        'cat_smooth': trial.suggest_uniform('cat_smooth', 5, 15),\n",
    "        'verbosity': -1,\n",
    "        'seed': 0\n",
    "    }\n",
    "\n",
    "    # FIXME: should I use here separate validation sets for early stopping and generalization estimate?\n",
    "    gbm = lgb.train(\n",
    "        params=params,\n",
    "        num_boost_round=5000,\n",
    "        train_set=lgb_data_eval,\n",
    "        valid_sets=[lgb_data_valid],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)],\n",
    "    )\n",
    "\n",
    "    valid_data_pred = gbm.predict(data=valid_data_x)\n",
    "    rmse = mean_squared_error(valid_data_y, valid_data_pred, squared=False)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "study_bo = optuna.create_study(\n",
    "    sampler=optuna.samplers.TPESampler(seed=0),\n",
    "    direction='minimize'\n",
    ")\n",
    "study_bo.optimize(objective, n_trials=200, n_jobs=4, show_progress_bar=True)\n",
    "\n",
    "print('Number of finished trials:', len(study_bo.trials))\n",
    "print('Best trial:', study_bo.best_trial.params)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = optuna.visualization.plot_optimization_history(study_bo)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = optuna.visualization.plot_intermediate_values(study_hb)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = optuna.visualization.plot_contour(study_hb)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.019983 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 13092\n",
      "[LightGBM] [Info] Number of data points in the train set: 7726, number of used features: 685\n",
      "[LightGBM] [Info] Start training from score 48.700467\n"
     ]
    }
   ],
   "source": [
    "# refit model on complete training dataset and check prediction error\n",
    "lgb_dataset = lgb.Dataset(\n",
    "    data=train_data_x,\n",
    "    label=train_data_y\n",
    ")\n",
    "\n",
    "bst = lgb.train(\n",
    "    params=study_bo.best_params, \n",
    "    train_set=lgb_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score 9.600447\n"
     ]
    }
   ],
   "source": [
    "test_data_preds = bst.predict(data=test_data_x)\n",
    "test_rmse = mean_squared_error(test_data_y, test_data_preds, squared=False)\n",
    "\n",
    "print('Score', round(test_rmse, ndigits=6))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hyper-parameter tuning with Hyperband informed by Hyperband.\n",
    "\n",
    "L. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, and A. Talwalkar, ‘Hyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization’. arXiv, Jun. 18, 2018. Accessed: Apr. 22, 2023. [Online]. Available: http://arxiv.org/abs/1603.06560"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of brackets 5\n"
     ]
    }
   ],
   "source": [
    "R = 5000/50\n",
    "num_brackets = math.floor(math.log(R, 3)) + 1\n",
    "print('Number of brackets', num_brackets)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# optuna hyper-parameter optimization with hyperband\n",
    "num_boost_round = 5000\n",
    "\n",
    "lgb_data_eval = lgb.Dataset(\n",
    "    data=eval_data_x, label=eval_data_y, free_raw_data=False)\n",
    "lgb_data_valid = lgb.Dataset(\n",
    "    data=valid_data_x, label=valid_data_y, reference=lgb_data_eval, free_raw_data=False)\n",
    "lgb_data_test = lgb.Dataset(\n",
    "    data=test_data_x, label=test_data_y, reference=lgb_data_eval)\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'learning_rate': 0.01,\n",
    "        # 'lambda_l1': trial.suggest_loguniform('lambda_l1', 1e-4, 100.0),\n",
    "        # 'lambda_l2': trial.suggest_loguniform('lambda_l2', 1e-4, 100.0),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 16, 512, log=True),\n",
    "        # 'feature_fraction': trial.suggest_uniform('feature_fraction', 0.6, 1.0),\n",
    "        # 'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.6, 1.0),\n",
    "        'cat_l2': trial.suggest_uniform('cat_l2', 5, 15),\n",
    "        'cat_smooth': trial.suggest_uniform('cat_smooth', 5, 15),\n",
    "        'verbosity': -1,\n",
    "        'seed': 0\n",
    "    }\n",
    "\n",
    "    # init model and pruning mechanism\n",
    "    gbm = lgb.train(\n",
    "        params=params,\n",
    "        num_boost_round=50,\n",
    "        train_set=lgb_data_eval,\n",
    "        valid_sets=[lgb_data_valid],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],\n",
    "        keep_training_booster=True\n",
    "    )\n",
    "\n",
    "    for step in range(num_boost_round):\n",
    "        gbm = lgb.train(\n",
    "            params=params,\n",
    "            num_boost_round=num_boost_round,\n",
    "            train_set=lgb_data_eval,\n",
    "            valid_sets=[lgb_data_valid],\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)],\n",
    "            keep_training_booster=True,\n",
    "            init_model=gbm\n",
    "        )\n",
    "\n",
    "        intermediate_pred = gbm.predict(\n",
    "            data=valid_data_x, num_iteration=gbm.best_iteration)\n",
    "        intermediate_rmse = mean_squared_error(\n",
    "            valid_data_y, intermediate_pred, squared=False)\n",
    "        \n",
    "        trial.report(intermediate_rmse, step)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "\n",
    "    valid_data_pred = gbm.predict(\n",
    "        data=valid_data_x, num_iteration=gbm.best_iteration)\n",
    "    rmse = mean_squared_error(valid_data_y, valid_data_pred, squared=False)\n",
    "\n",
    "    return rmse\n",
    "\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.INFO)\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=0)\n",
    "pruner = optuna.pruners.HyperbandPruner(\n",
    "    min_resource=50, max_resource=num_boost_round, reduction_factor=3)\n",
    "\n",
    "study_hb = optuna.create_study(\n",
    "    sampler=sampler, pruner=pruner, direction='minimize')\n",
    "study_hb.optimize(objective, n_trials=20, show_progress_bar=True)\n",
    "\n",
    "print('Number of finished trials:', len(study_hb.trials))\n",
    "print('Best trial:', study_hb.best_trial.params)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = optuna.visualization.plot_optimization_history(study_hb)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = optuna.visualization.plot_intermediate_values(study_hb)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fig = optuna.visualization.plot_contour(study_hb)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Experimental: Optimisation by Evolutionary Hyperband \n",
    "\n",
    "N. Awad, N. Mallik, and F. Hutter, ‘DEHB: Evolutionary Hyperband for Scalable, Robust and Efficient Hyperparameter Optimization’. arXiv, Oct. 21, 2021. Accessed: Apr. 22, 2023. [Online]. Available: http://arxiv.org/abs/2105.09821"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration space object:\n",
      "  Hyperparameters:\n",
      "    bagging_fraction, Type: UniformFloat, Range: [0.5, 1.0], Default: 1.0\n",
      "    cat_l2, Type: UniformFloat, Range: [5.0, 15.0], Default: 10.0\n",
      "    cat_smooth, Type: UniformFloat, Range: [5.0, 15.0], Default: 10.0\n",
      "    feature_fraction, Type: UniformFloat, Range: [0.5, 1.0], Default: 1.0\n",
      "    lambda_l1, Type: UniformFloat, Range: [0.0001, 10.0], Default: 0.0001, on log-scale\n",
      "    lambda_l2, Type: UniformFloat, Range: [0.0001, 10.0], Default: 0.0001, on log-scale\n",
      "    learning_rate, Type: UniformFloat, Range: [0.0001, 1.0], Default: 0.01, on log-scale\n",
      "    num_leaves, Type: UniformInteger, Range: [16, 256], Default: 32, on log-scale\n",
      "\n",
      "Dimensionality of search space: 8\n"
     ]
    }
   ],
   "source": [
    "# define fidelity range\n",
    "min_budget, max_budget = 50, 5000\n",
    "\n",
    "# define config space\n",
    "def create_search_space(seed=0):\n",
    "    \"\"\"Parameter space to be optimized --- contains the hyperparameters\"\"\"\n",
    "    cs = CS.ConfigurationSpace(seed=seed)\n",
    "\n",
    "    cs.add_hyperparameters([\n",
    "        # CS.UniformFloatHyperparameter('learning_rate', lower=1e-4, upper=1.0, default_value=1e-2, log=True),\n",
    "        # CS.UniformFloatHyperparameter('lambda_l1', lower=1e-4, upper=10.0, default_value=1e-4, log=True),\n",
    "        # CS.UniformFloatHyperparameter('lambda_l2', lower=1e-4, upper=10.0, default_value=1e-4, log=True),\n",
    "        CS.UniformFloatHyperparameter('feature_fraction', lower=0.6, upper=1.0, default_value=1.0, log=False),\n",
    "        CS.UniformFloatHyperparameter('bagging_fraction', lower=0.6, upper=1.0, default_value=1.0, log=False),\n",
    "        CS.UniformIntegerHyperparameter('num_leaves', lower=16, upper=256, default_value=32, log=False),\n",
    "        CS.UniformFloatHyperparameter('cat_l2', lower=5.0, upper=15.0, default_value=10.0, log=False),\n",
    "        CS.UniformFloatHyperparameter('cat_smooth', lower=5.0, upper=15.0, default_value=10.0, log=False)\n",
    "    ])\n",
    "\n",
    "    return cs\n",
    "\n",
    "cs = create_search_space(seed=0)\n",
    "print(cs)\n",
    "\n",
    "dimensions = len(cs.get_hyperparameters())\n",
    "print(\"Dimensionality of search space: {}\".format(dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define eval - earlys - valid data sets\n",
    "evalearlys_data_x, valid_data_x, evalearlys_data_y, valid_data_y = train_test_split(\n",
    "    train_data_x,\n",
    "    train_data_y,\n",
    "    train_size=0.9,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "eval_data_x, earlys_data_x, eval_data_y, earlys_data_y = train_test_split(\n",
    "    evalearlys_data_x,\n",
    "    evalearlys_data_y,\n",
    "    train_size=0.9,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "lgb_data_eval = lgb.Dataset(data=eval_data_x, label=eval_data_y)\n",
    "lgb_data_earlys = lgb.Dataset(\n",
    "    data=earlys_data_x, label=earlys_data_y, reference=lgb_data_eval)\n",
    "lgb_data_valid = lgb.Dataset(\n",
    "    data=valid_data_x, label=valid_data_y, reference=lgb_data_eval)\n",
    "lgb_data_test = lgb.Dataset(\n",
    "    data=test_data_x, label=test_data_y, reference=lgb_data_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function interface with DEHB\n",
    "def target_function(config, budget, **kwargs):\n",
    "    # extracting support information\n",
    "    max_budget = kwargs[\"max_budget\"]\n",
    "\n",
    "    if budget is None:\n",
    "        budget = max_budget\n",
    "\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        'boosting': 'gbdt',\n",
    "        'verbosity': -1,\n",
    "        'seed': 0\n",
    "    }\n",
    "\n",
    "    params.update(config.get_dictionary())\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    gbm = lgb.train(\n",
    "        params=params,\n",
    "        num_boost_round=int(budget),\n",
    "        train_set=lgb_data_eval,\n",
    "        valid_sets=[lgb_data_earlys],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=10, verbose=False)],\n",
    "        keep_training_booster=True\n",
    "    )\n",
    "\n",
    "    cost = time.time() - start\n",
    "\n",
    "    valid_data_pred = gbm.predict(valid_data_x, num_iteration=gbm.best_iteration)\n",
    "    valid_rmse = mean_squared_error(\n",
    "        valid_data_y, valid_data_pred, squared=False)\n",
    "\n",
    "    test_data_pred = gbm.predict(test_data_x, num_iteration=gbm.best_iteration)\n",
    "    test_rmse = mean_squared_error(test_data_y, test_data_pred, squared=False)\n",
    "\n",
    "    best_iteration = gbm.best_iteration\n",
    "    \n",
    "    result = {\n",
    "        \"fitness\": valid_rmse,  \n",
    "        \"cost\": cost,\n",
    "        \"info\": {\n",
    "            \"test_score\": test_rmse,\n",
    "            \"budget\": budget, \n",
    "            \"best_iteration\": best_iteration\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define tuning algorithm (hyper-hyperparams are default)\n",
    "dehb = DEHB(\n",
    "    f=target_function,\n",
    "    cs=cs,\n",
    "    dimensions=dimensions,\n",
    "    min_budget=min_budget,\n",
    "    max_budget=max_budget,\n",
    "    eta=3, \n",
    "    strategy='rand1_bin',\n",
    "    mutation_factor=0.5,\n",
    "    crossover_prob=0.5,\n",
    "    n_workers=4,\n",
    "    output_path=\"./temp\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2023-04-22 23:16:18.621\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mdehb.optimizers.dehb\u001b[0m:\u001b[36mreset\u001b[0m:\u001b[36m107\u001b[0m - \u001b[1m\n",
      "\n",
      "RESET at 04/22/23 23:16:18 W. Europe Summer Time\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-22 23:16:19,014 - distributed.nanny - WARNING - Restarting worker\n",
      "2023-04-22 23:16:19,045 - distributed.nanny - WARNING - Restarting worker\n",
      "2023-04-22 23:16:19,053 - distributed.nanny - WARNING - Restarting worker\n",
      "2023-04-22 23:16:19,060 - distributed.nanny - WARNING - Restarting worker\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# run tuning algorithm\u001b[39;00m\n\u001b[0;32m      2\u001b[0m dehb\u001b[39m.\u001b[39mreset()\n\u001b[1;32m----> 4\u001b[0m trajectory, runtime, history \u001b[39m=\u001b[39m dehb\u001b[39m.\u001b[39;49mrun(\n\u001b[0;32m      5\u001b[0m     total_cost\u001b[39m=\u001b[39;49m\u001b[39m60\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m60\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39m6\u001b[39;49m, \n\u001b[0;32m      6\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m      7\u001b[0m     max_budget\u001b[39m=\u001b[39;49mdehb\u001b[39m.\u001b[39;49mmax_budget, \n\u001b[0;32m      8\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\loguru\\_logger.py:1251\u001b[0m, in \u001b[0;36mLogger.catch.<locals>.Catcher.__call__.<locals>.catch_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1249\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcatch_wrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1250\u001b[0m     \u001b[39mwith\u001b[39;00m catcher:\n\u001b[1;32m-> 1251\u001b[0m         \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1252\u001b[0m     \u001b[39mreturn\u001b[39;00m default\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\dehb\\optimizers\\dehb.py:752\u001b[0m, in \u001b[0;36mDEHB.run\u001b[1;34m(self, fevals, brackets, total_cost, single_node_with_gpus, verbose, debug, save_intermediate, save_history, name, **kwargs)\u001b[0m\n\u001b[0;32m    750\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_run_budget_exhausted(fevals, brackets, total_cost):\n\u001b[0;32m    751\u001b[0m     \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m--> 752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mis_worker_available():\n\u001b[0;32m    753\u001b[0m     job_info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_next_job()\n\u001b[0;32m    754\u001b[0m     \u001b[39mif\u001b[39;00m brackets \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m job_info[\u001b[39m'\u001b[39m\u001b[39mbracket_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m brackets:\n\u001b[0;32m    755\u001b[0m         \u001b[39m# ignore submission and only collect results\u001b[39;00m\n\u001b[0;32m    756\u001b[0m         \u001b[39m# when brackets are chosen as run budget, an extra bracket is created\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    760\u001b[0m         \u001b[39m# _is_run_budget_exhausted() will not return True until all the lower brackets\u001b[39;00m\n\u001b[0;32m    761\u001b[0m         \u001b[39m# have finished computation and returned its results\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\dehb\\optimizers\\dehb.py:405\u001b[0m, in \u001b[0;36mDEHB.is_worker_available\u001b[1;34m(self, verbose)\u001b[0m\n\u001b[0;32m    401\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[39m# checks the absolute number of workers mapped to the client scheduler\u001b[39;00m\n\u001b[0;32m    403\u001b[0m \u001b[39m# client.ncores() should return a dict with the keys as unique addresses to these workers\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[39m# treating the number of available workers in this manner\u001b[39;00m\n\u001b[1;32m--> 405\u001b[0m workers \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_worker_count()  \u001b[39m# len(self.client.ncores())\u001b[39;00m\n\u001b[0;32m    406\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfutures) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m workers:\n\u001b[0;32m    407\u001b[0m     \u001b[39m# pause/wait if active worker count greater allocated workers\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\dehb\\optimizers\\dehb.py:392\u001b[0m, in \u001b[0;36mDEHB._get_worker_count\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_worker_count\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    391\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient, Client):\n\u001b[1;32m--> 392\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mncores())\n\u001b[0;32m    393\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    394\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\distributed\\client.py:3857\u001b[0m, in \u001b[0;36mClient.nthreads\u001b[1;34m(self, workers, **kwargs)\u001b[0m\n\u001b[0;32m   3855\u001b[0m \u001b[39mif\u001b[39;00m workers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(workers, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m, \u001b[39mset\u001b[39m)):\n\u001b[0;32m   3856\u001b[0m     workers \u001b[39m=\u001b[39m [workers]\n\u001b[1;32m-> 3857\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msync(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscheduler\u001b[39m.\u001b[39mncores, workers\u001b[39m=\u001b[39mworkers, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\distributed\\utils.py:349\u001b[0m, in \u001b[0;36mSyncMethodMixin.sync\u001b[1;34m(self, func, asynchronous, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    347\u001b[0m     \u001b[39mreturn\u001b[39;00m future\n\u001b[0;32m    348\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\n\u001b[0;32m    350\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloop, func, \u001b[39m*\u001b[39margs, callback_timeout\u001b[39m=\u001b[39mcallback_timeout, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[0;32m    351\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\distributed\\utils.py:412\u001b[0m, in \u001b[0;36msync\u001b[1;34m(loop, func, callback_timeout, *args, **kwargs)\u001b[0m\n\u001b[0;32m    410\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    411\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m e\u001b[39m.\u001b[39mis_set():\n\u001b[1;32m--> 412\u001b[0m         wait(\u001b[39m10\u001b[39;49m)\n\u001b[0;32m    414\u001b[0m \u001b[39mif\u001b[39;00m error:\n\u001b[0;32m    415\u001b[0m     typ, exc, tb \u001b[39m=\u001b[39m error\n",
      "File \u001b[1;32mc:\\Users\\Daniel\\Documents\\GitHub\\zindi-early-learning-predictors-py\\.venv\\lib\\site-packages\\distributed\\utils.py:401\u001b[0m, in \u001b[0;36msync.<locals>.wait\u001b[1;34m(timeout)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(timeout):\n\u001b[0;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[39mreturn\u001b[39;00m e\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    402\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         loop\u001b[39m.\u001b[39madd_callback(cancel)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:607\u001b[0m, in \u001b[0;36mEvent.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    605\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[0;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[1;32m--> 607\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    608\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[0;32m    325\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# run tuning algorithm\n",
    "dehb.reset()\n",
    "\n",
    "trajectory, runtime, history = dehb.run(\n",
    "    total_cost=None, \n",
    "    fevals=100,\n",
    "    verbose=False,\n",
    "    max_budget=dehb.max_budget, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212 212 212\n",
      "\n",
      "Last evaluated configuration, \n",
      "Configuration(values={\n",
      "  'bagging_fraction': 0.6340546324562695,\n",
      "  'cat_l2': 9.618534421619371,\n",
      "  'cat_smooth': 6.806606215406344,\n",
      "  'feature_fraction': 0.63399434125221,\n",
      "  'lambda_l1': 8.275173171359791,\n",
      "  'lambda_l2': 0.0012553755831805511,\n",
      "  'learning_rate': 0.0001792569563679823,\n",
      "  'num_leaves': 107,\n",
      "})\n",
      "got a score of 10.33612642703613, was evaluated at a budget of 10000.00 and took 481.239 seconds to run.\n",
      "The additional info attached: {'test_score': 10.123112749160718, 'budget': 10000.0, 'best_iteration': 10000}\n",
      "\n",
      "Best evaluated configuration, \n",
      "Configuration(values={\n",
      "  'bagging_fraction': 0.8088284912846216,\n",
      "  'cat_l2': 8.034264947167088,\n",
      "  'cat_smooth': 9.075255369543438,\n",
      "  'feature_fraction': 0.522285555493832,\n",
      "  'lambda_l1': 0.034076843234482956,\n",
      "  'lambda_l2': 0.0077864425839706776,\n",
      "  'learning_rate': 0.018365651651009347,\n",
      "  'num_leaves': 64,\n",
      "})\n",
      "got a score of 9.625861223941522.\n",
      "The additional info attached: {'test_score': 9.568879269018263, 'budget': 3333.333333333333, 'best_iteration': 419}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(trajectory), len(runtime), len(history), end=\"\\n\\n\")\n",
    "\n",
    "# last recorded function evaluation\n",
    "last_eval = history[-1]\n",
    "config, score, cost, budget, _info = last_eval\n",
    "\n",
    "print(\"Last evaluated configuration, \")\n",
    "print(dehb.vector_to_configspace(config), end=\"\")\n",
    "print(\"got a score of {}, was evaluated at a budget of {:.2f} and \"\n",
    "      \"took {:.3f} seconds to run.\".format(score, budget, cost))\n",
    "print(\"The additional info attached: {}\".format(_info), end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "print(\"Best evaluated configuration, \")\n",
    "print(dehb.vector_to_configspace(dehb.inc_config), end=\"\")\n",
    "print(\"got a score of {}.\".format(dehb.inc_score))\n",
    "print(\"The additional info attached: {}\".format(dehb.inc_info), end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Final model fit with tuned params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a final model to complete training set and predict submission data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load model\n",
    "# bst.save_model('model.txt')\n",
    "# bst = lgb.Booster(model_file='model.txt')  # init model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
